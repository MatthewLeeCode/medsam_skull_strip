{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from os.path import join\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from segment_anything import SamPredictor, sam_model_registry\n",
    "from segment_anything.utils.transforms import ResizeLongestSide\n",
    "from utils.SurfaceDice import compute_dice_coefficient\n",
    "from sklearn.model_selection import train_test_split\n",
    "import radvis as rv\n",
    "import monai\n",
    "\n",
    "torch.manual_seed(2023)\n",
    "np.random.seed(2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = \"C:/Users/matth/Data/brainseg/nfbs_dataset_preprocessed\"\n",
    "IMAGES_FOLDER = join(DATA_LOCATION, \"images\")\n",
    "MASKS_FOLDER = join(DATA_LOCATION, \"masks\")\n",
    "EMBEDDINGS_FOLDER = join(DATA_LOCATION, \"embeddings\")\n",
    "DATA_SPLIT = 0.2\n",
    "MODEL_VERSION = \"0.0.1\"\n",
    "MODEL_TASK = \"brainstrip\"\n",
    "MODEL_SAVE_PATH = f\"models/{MODEL_TASK}/{MODEL_VERSION}\"\n",
    "\n",
    "# Create folder for save path\n",
    "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ids = [i for i in os.listdir(EMBEDDINGS_FOLDER) if i.endswith(\".npz\")]\n",
    "# Train test split\n",
    "train_ids, test_ids = train_test_split(embedding_ids, test_size=DATA_SPLIT, random_state=2023)\n",
    "train_filenames = [join(EMBEDDINGS_FOLDER, i) for i in train_ids]\n",
    "test_filenames = [join(EMBEDDINGS_FOLDER, i) for i in test_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 training samples\n",
      "25 testing samples\n"
     ]
    }
   ],
   "source": [
    "print(f\"{len(train_ids)} training samples\")\n",
    "print(f\"{len(test_ids)} testing samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class NpzDataset(Dataset):\n",
    "    def __init__(self, data_filenames):\n",
    "        self.npz_files = data_filenames\n",
    "        self.cumulative_sizes = self.compute_cumulative_sizes()\n",
    "        self.last_loaded_file = None\n",
    "        self.embedding_cache = None\n",
    "        self.gts_cache = None\n",
    "\n",
    "    def compute_cumulative_sizes(self):\n",
    "        cumulative_sizes = []\n",
    "        total_size = 0\n",
    "        for f in self.npz_files:\n",
    "            data = np.load(f)\n",
    "            total_size += data['gts'].shape[0]\n",
    "            cumulative_sizes.append(total_size)\n",
    "        return cumulative_sizes\n",
    "\n",
    "    def find_file_index(self, index):\n",
    "        for i, size in enumerate(self.cumulative_sizes):\n",
    "            if index < size:\n",
    "                return i, index if i == 0 else index - self.cumulative_sizes[i-1]\n",
    "        raise IndexError(\"index out of range\")\n",
    "\n",
    "    def load_data(self, file_index):\n",
    "        if self.last_loaded_file == file_index:\n",
    "            return self.gts_cache, self.embedding_cache\n",
    "        else:\n",
    "            # Print the files completed\n",
    "            # Time the process\n",
    "            start = time.time()\n",
    "            data = np.load(self.npz_files[file_index])\n",
    "            self.embedding_cache = data['img_embeddings']\n",
    "            self.gts_cache = data['gts']\n",
    "            self.last_loaded_file = file_index\n",
    "            end = time.time()\n",
    "            #print(f\"Time to load: {end-start}\")\n",
    "            return self.gts_cache, self.embedding_cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.cumulative_sizes[-1]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        file_index, file_item_index = self.find_file_index(index)\n",
    "        # Time the process\n",
    "        \n",
    "        ori_data, embedding_data = self.load_data(file_index)\n",
    "        start = time.time()\n",
    "        \n",
    "        ori_gt = ori_data[file_item_index]\n",
    "        img_embedding = embedding_data[file_item_index]\n",
    "\n",
    "        y_indices, x_indices = np.where(ori_gt > 0)\n",
    "        x_min, x_max = np.min(x_indices), np.max(x_indices)\n",
    "        y_min, y_max = np.min(y_indices), np.max(y_indices)\n",
    "\n",
    "\n",
    "        # add perturbation to bounding box coordinates\n",
    "        H, W = ori_gt.shape\n",
    "        x_min = max(0, x_min - np.random.randint(0, 20))\n",
    "        x_max = min(W, x_max + np.random.randint(0, 20))\n",
    "        y_min = max(0, y_min - np.random.randint(0, 20))\n",
    "        y_max = min(H, y_max + np.random.randint(0, 20))\n",
    "\n",
    "        bboxes = np.array([x_min, y_min, x_max, y_max])\n",
    "\n",
    "        # Print time before the tensor converts\n",
    "        # convert img embedding, mask, bounding box to torch tensor\n",
    "        result = (torch.tensor(img_embedding).float(), torch.tensor(ori_gt[None, :,:]).long(), torch.tensor(bboxes).float())\n",
    "        end = time.time()\n",
    "        #print(f\"Time to get item: {end-start}\")\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_embed.shape=torch.Size([8, 256, 64, 64]), gt2D.shape=torch.Size([8, 1, 256, 256]), bboxes.shape=torch.Size([8, 4])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "demo_dataset = NpzDataset(train_filenames)\n",
    "demo_dataloader = DataLoader(demo_dataset, batch_size=8, shuffle=False)\n",
    "for img_embed, gt2D, bboxes in demo_dataloader:\n",
    "    # img_embed: (B, 256, 64, 64), gt2D: (B, 1, 256, 256), bboxes: (B, 4)\n",
    "    print(f\"{img_embed.shape=}, {gt2D.shape=}, {bboxes.shape=}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare SAM model\n",
    "model_type = 'vit_b'\n",
    "checkpoint = 'work_dir/SAM/sam_vit_b_01ec64.pth'\n",
    "device = 'cuda:0'\n",
    "sam_model = sam_model_registry[model_type](checkpoint=checkpoint).to(device)\n",
    "sam_model.train()\n",
    "\n",
    "# Set up the optimizer, hyperparameter tuning will improve performance here\n",
    "optimizer = torch.optim.Adam(sam_model.mask_decoder.parameters(), lr=1e-5, weight_decay=0)\n",
    "seg_loss = monai.losses.DiceCELoss(sigmoid=True, squared_pred=True, reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3809ed779af94232a1afb9c9740eb235",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 0, Loss: 20.15462022088468\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2f76efb2b0249719f3942cd60ba0ef7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 1, Loss: 14.68604763224721\n",
      "Epoch 3/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54a75b8308ae411ea70a5175d507005c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 2, Loss: 13.533539861440659\n",
      "Epoch 4/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "349688ca37cd41d09ca4b821784d6784",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 3, Loss: 12.809670984745026\n",
      "Epoch 5/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86eb75f7ed784edd845ee8a2d511a822",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH: 4, Loss: 12.383253177627921\n",
      "Epoch 6/100\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea3efd942ef43a1ba2c94ae8fe7d213",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/518 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "from tqdm.notebook import tqdm as tqdm_notebook\n",
    "\n",
    "num_epochs = 100\n",
    "losses = []\n",
    "best_loss = 1e10\n",
    "train_dataset = NpzDataset(train_filenames)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    # train\n",
    "    for image_embedding, gt2D, boxes in tqdm_notebook(train_dataloader):\n",
    "        # do not compute gradients for image encoder and prompt encoder\n",
    "        with torch.no_grad():\n",
    "            # convert box to 1024x1024 grid\n",
    "            box_np = boxes.numpy()\n",
    "            sam_trans = ResizeLongestSide(sam_model.image_encoder.img_size)\n",
    "            box = sam_trans.apply_boxes(box_np, (gt2D.shape[-2], gt2D.shape[-1]))\n",
    "            box_torch = torch.as_tensor(box, dtype=torch.float, device=device)\n",
    "            if len(box_torch.shape) == 2:\n",
    "                box_torch = box_torch[:, None, :] # (B, 1, 4)\n",
    "            # get prompt embeddings \n",
    "            sparse_embeddings, dense_embeddings = sam_model.prompt_encoder(\n",
    "                points=None,\n",
    "                boxes=box_torch,\n",
    "                masks=None,\n",
    "            )\n",
    "        # predicted masks\n",
    "        mask_predictions, _ = sam_model.mask_decoder(\n",
    "            image_embeddings=image_embedding.to(device), # (B, 256, 64, 64)\n",
    "            image_pe=sam_model.prompt_encoder.get_dense_pe(), # (1, 256, 64, 64)\n",
    "            sparse_prompt_embeddings=sparse_embeddings, # (B, 2, 256)\n",
    "            dense_prompt_embeddings=dense_embeddings, # (B, 256, 64, 64)\n",
    "            multimask_output=False,\n",
    "          )\n",
    "\n",
    "        loss = seg_loss(mask_predictions, gt2D.to(device))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        step += 1\n",
    "\n",
    "    losses.append(epoch_loss)\n",
    "    print(f'EPOCH: {epoch}, Loss: {epoch_loss}')\n",
    "    # save the latest model checkpoint\n",
    "    torch.save(sam_model.state_dict(), join(MODEL_SAVE_PATH, 'sam_model_latest.pth'))\n",
    "    # save the best model\n",
    "    if epoch_loss < best_loss:\n",
    "        best_loss = epoch_loss\n",
    "        torch.save(sam_model.state_dict(), join(MODEL_SAVE_PATH, 'sam_model_best.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "medsam",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
